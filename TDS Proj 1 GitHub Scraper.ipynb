{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"gRB_h3l_byY9","outputId":"96b83386-fbb6-4865-82e9-1afad7d5bbbb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Scraped 481 users and 29406 repositories\n"]}],"source":["import requests\n","import pandas as pd\n","import time\n","import logging\n","from typing import List, Dict, Any\n","\n","class GitHubScraper:\n","    def __init__(self, token: str):\n","        \"\"\"\n","        Initialize the GitHub scraper with your API token.\n","\n","        Args:\n","            token (str): GitHub Personal Access Token\n","        \"\"\"\n","        self.headers = {\n","            'Authorization': f'token {token}',\n","            'Accept': 'application/vnd.github.v3+json'\n","        }\n","        self.base_url = 'https://api.github.com'\n","\n","       # Setup logging\n","        logging.basicConfig(\n","            level=logging.INFO,\n","            format='%(asctime)s - %(levelname)s - %(message)s'\n","        )\n","        self.logger = logging.getLogger(__name__)\n","\n","    def _make_request(self, url: str, params: dict = None) -> Dict:\n","        \"\"\"\n","        Make a request to the GitHub API with rate limit handling.\n","        \"\"\"\n","        while True:\n","            response = requests.get(url, headers=self.headers, params=params)\n","\n","            if response.status_code == 200:\n","                return response.json()\n","            elif response.status_code == 403:\n","                reset_time = int(response.headers.get('X-RateLimit-Reset', 0))\n","                sleep_time = max(reset_time - time.time(), 0) + 1\n","                self.logger.warning(f\"Rate limit hit. Sleeping for {sleep_time} seconds\")\n","                time.sleep(sleep_time)\n","            else:\n","                self.logger.error(f\"Error {response.status_code}: {response.text}\")\n","                response.raise_for_status()\n","\n","    def clean_company_name(self, company: str) -> str:\n","        \"\"\"\n","        Clean up company names according to specifications.\n","        \"\"\"\n","        if not company:\n","            return \"\"\n","\n","        # Strip whitespace and @ symbol\n","        cleaned = company.strip().lstrip('@')\n","\n","        # Convert to uppercase\n","        return cleaned.upper()\n","\n","    def search_users(self, location: str, min_followers: int) -> List[Dict]:\n","        \"\"\"\n","        Search for GitHub users in a specific location with minimum followers.\n","        \"\"\"\n","        users = []\n","        page = 1\n","\n","        while True:\n","            self.logger.info(f\"Fetching users page {page}\")\n","\n","            query = f\"location:{location} followers:>={min_followers}\"\n","            params = {\n","                'q': query,\n","                'per_page': 100,\n","                'page': page\n","            }\n","\n","            url = f\"{self.base_url}/search/users\"\n","            response = self._make_request(url, params)\n","\n","            if not response['items']:\n","                break\n","\n","            for user in response['items']:\n","                user_data = self._make_request(user['url'])\n","\n","                # Extract only the required fields with exact matching names\n","                cleaned_data = {\n","                    'login': user_data['login'],\n","                    'name': user_data['name'] if user_data['name'] else \"\",\n","                    'company': self.clean_company_name(user_data.get('company')),\n","                    'location': user_data['location'] if user_data['location'] else \"\",\n","                    'email': user_data['email'] if user_data['email'] else \"\",\n","                    'hireable': user_data['hireable'] if user_data['hireable'] is not None else False,\n","                    'bio': user_data['bio'] if user_data['bio'] else \"\",\n","                    'public_repos': user_data['public_repos'],\n","                    'followers': user_data['followers'],\n","                    'following': user_data['following'],\n","                    'created_at': user_data['created_at']\n","                }\n","\n","                users.append(cleaned_data)\n","\n","            page += 1\n","\n","        return users\n","\n","    def get_user_repositories(self, username: str, max_repos: int = 500) -> List[Dict]:\n","        \"\"\"\n","        Get repositories for a specific user.\n","        \"\"\"\n","        repos = []\n","        page = 1\n","\n","        while len(repos) < max_repos:\n","            self.logger.info(f\"Fetching repositories for {username}, page {page}\")\n","\n","            params = {\n","                'sort': 'pushed',\n","                'direction': 'desc',\n","                'per_page': 100,\n","                'page': page\n","            }\n","\n","            url = f\"{self.base_url}/users/{username}/repos\"\n","            response = self._make_request(url, params)\n","\n","            if not response:\n","                break\n","\n","            for repo in response:\n","                # Extract only the required fields with exact matching names\n","                repo_data = {\n","                    'login': username,  # Adding owner's login as required\n","                    'full_name': repo['full_name'],\n","                    'created_at': repo['created_at'],\n","                    'stargazers_count': repo['stargazers_count'],\n","                    'watchers_count': repo['watchers_count'],\n","                    'language': repo['language'] if repo['language'] else \"\",\n","                    'has_projects': repo['has_projects'],\n","                    'has_wiki': repo['has_wiki'],\n","                    'license_name': repo['license']['key'] if repo.get('license') else \"\"\n","                }\n","\n","                repos.append(repo_data)\n","\n","            if len(response) < 100:\n","                break\n","\n","            page += 1\n","\n","        return repos[:max_repos]\n","\n","def main():\n","    # Personal Access Github Token\n","    token = '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'\n","    # Initialize scraper\n","    scraper = GitHubScraper(token)\n","\n","    # Search for users in Zurich with >50 followers\n","    users = scraper.search_users(location='Zurich', min_followers=50)\n","\n","    # Save users to CSV\n","    users_df = pd.DataFrame(users)\n","    users_df.to_csv('users.csv', index=False)\n","\n","    # Get repositories for each user\n","    all_repos = []\n","    for user in users:\n","        repos = scraper.get_user_repositories(user['login'])\n","        all_repos.extend(repos)\n","\n","    # Save repositories to CSV\n","    repos_df = pd.DataFrame(all_repos)\n","    repos_df.to_csv('repositories.csv', index=False)\n","\n","    print(f\"Scraped {len(users)} users and {len(all_repos)} repositories\")\n","\n","    # Create README.md\n","    with open('README.md', 'w') as f:\n","        f.write(f\"\"\"# GitHub Users in Zurich\"\"\")\n","\n","# This repository contains data about GitHub users in Zurich with over 50 followers and their repositories.\n","# 1. `users.csv`: Contains information about {len(users)} GitHub users in Zurich with over 50 followers\n","# 2. `repositories.csv`: Contains information about {len(all_repos)}\n","\n","if __name__ == \"__main__\":\n","    main()"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOHLKrwmlmY1unS6eyf/4Ao"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}